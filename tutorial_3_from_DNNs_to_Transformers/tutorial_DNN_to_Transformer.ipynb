{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: From Deep Neural Networks to Transformers\n",
    "\n",
    "## Architecture Selection for Time-Series Forecasting\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand how different architectures process sequential data\n",
    "- Compare MLP, CNN, and Transformer performance on time-series\n",
    "- Learn when CNNs excel vs when Transformers are needed\n",
    "- Discover architectural design principles for sequential modeling\n",
    "\n",
    "**Duration:** 90-120 minutes\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this tutorial, you'll implement and compare three fundamentally different neural network architectures for time-series forecasting:\n",
    "\n",
    "1. **Multi-Layer Perceptron (MLP)** - Treats data as unstructured features\n",
    "2. **Convolutional Neural Network (CNN)** - Exploits local temporal patterns\n",
    "3. **Transformer** - Uses global attention across all time steps\n",
    "\n",
    "Through a specially designed multi-scale wave dataset, you'll discover:\n",
    "- **Fast oscillations (period ~7 steps)** ‚Üí CNN captures perfectly!\n",
    "- **Slow trends (period ~100 steps)** ‚Üí Transformer needed\n",
    "- **Both together** ‚Üí Shows architecture strengths/weaknesses\n",
    "\n",
    "This is the same architectural progression that powers modern AI: from simple feedforward networks to the attention mechanisms in GPT!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Configuration\n",
    "\n",
    "First, let's import all necessary libraries and configure our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import fft, fftfreq\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Tutorial-specific imports\n",
    "from DNNs_to_Transformer_tutorial.logger import configure_logging\n",
    "from DNNs_to_Transformer_tutorial.data_generator_multiscale import MultiScaleWaveDataset\n",
    "from DNNs_to_Transformer_tutorial.MultiLayerPerceptron_AR import MultiLayerPerceptronAR\n",
    "from DNNs_to_Transformer_tutorial.CNN_AR_v2 import CNN_AR_v2\n",
    "from DNNs_to_Transformer_tutorial.Transformer_AR import TransformerAR\n",
    "\n",
    "# Configure logging\n",
    "configure_logging()\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# For better plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters and Configuration\n",
    "\n",
    "These parameters control:\n",
    "- **Data generation**: Time steps, spatial samples, amplitudes\n",
    "- **Training**: Batch size, epochs, learning rate\n",
    "- **Forecasting**: History length, forecast horizon\n",
    "\n",
    "Feel free to experiment with these values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HYPERPARAMETERS - Experiment with these!\n",
    "# ============================================================================\n",
    "\n",
    "# Data parameters\n",
    "T_total = 200          # Total time steps in generated sequence\n",
    "T_history = 150        # History length (input to models)\n",
    "H_forecast = 20        # Forecast horizon (how many steps to predict)\n",
    "L_spatial = 100        # Number of spatial samples (polynomial evaluations)\n",
    "x_range = (-2.0, 2.0)  # Spatial range\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 128       # Batch size for training\n",
    "n_train = 1000         # Number of training samples\n",
    "n_val = 100            # Number of validation samples\n",
    "n_test = 100           # Number of test samples\n",
    "n_epochs = 50          # Maximum training epochs\n",
    "learning_rate = 0.001  # Learning rate\n",
    "\n",
    "# Training mode\n",
    "USE_DIRECT_TRAINING = False  # False = Teacher forcing (better results!)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TUTORIAL 3: ARCHITECTURE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Data: {n_train} train, {n_val} val, {n_test} test samples\")\n",
    "print(f\"Sequence: {T_history} history ‚Üí {H_forecast} forecast\")\n",
    "print(f\"Training: {n_epochs} epochs max, lr={learning_rate}\")\n",
    "print(f\"Mode: {'Teacher Forcing' if not USE_DIRECT_TRAINING else 'Direct Training'}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding the Multi-Scale Wave Dataset\n",
    "\n",
    "### Why This Dataset?\n",
    "\n",
    "We designed a dataset with **multiple timescales** to test architecture capabilities:\n",
    "\n",
    "$$\n",
    "y(x,t) = \\underbrace{0.5 \\sin(2\\pi t/100)}_{\\text{Slow trend}} + \\underbrace{0.8 \\sin(2\\pi(t/7 + x/4))}_{\\text{Fast wave}} + \\underbrace{\\text{regime shifts}}_{\\text{Medium}} + \\text{noise}\n",
    "$$\n",
    "\n",
    "**Components:**\n",
    "1. **Slow Trend** (period ~100 steps) - Tests long-range dependencies\n",
    "2. **Fast Wave** (period ~7 steps) - Tests local pattern detection\n",
    "3. **Regime Shifts** (every ~30 steps) - Medium-scale complexity\n",
    "4. **Spatial Correlation** - Tests spatial pattern handling\n",
    "\n",
    "### Expected Architecture Performance\n",
    "\n",
    "- **MLP**: Struggles (no structure exploitation)\n",
    "- **CNN**: Captures fast wave (kernel_size=7 matches!), misses slow trend\n",
    "- **Transformer**: Captures all scales (global attention)\n",
    "\n",
    "Let's create and visualize this dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the multi-scale wave dataset\n",
    "dataset = MultiScaleWaveDataset(\n",
    "    T=T_total,\n",
    "    L=L_spatial,\n",
    "    x_range=x_range,\n",
    "    noise_std=0.1,         # Noise level\n",
    "    slow_amplitude=0.5,    # Slow trend strength\n",
    "    fast_amplitude=0.8,    # Fast wave strength (CNN-friendly!)\n",
    "    regime_amplitude=0.3,  # Regime shift strength\n",
    "    spatial_amplitude=0.2, # Spatial correlation strength\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Dataset created!\")\n",
    "print(\"\\nDataset configuration:\")\n",
    "print(f\"  - Slow trend amplitude: 0.5 (period ~100 steps)\")\n",
    "print(f\"  - Fast wave amplitude: 0.8 (period ~7 steps) ‚Üê CNN sweet spot!\")\n",
    "print(f\"  - Regime shifts: every ~30 steps\")\n",
    "print(f\"  - Noise: std=0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Multi-Scale System\n",
    "\n",
    "Let's generate one sample and examine its components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate one sample with individual components\n",
    "x_sample, y_sample, components = dataset.generate_sequence(return_components=True)\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# ==================================================\n",
    "# Top Left: Full Heatmap (Space-Time Evolution)\n",
    "# ==================================================\n",
    "ax = axes[0, 0]\n",
    "im = ax.imshow(\n",
    "    y_sample.T.numpy(),\n",
    "    aspect='auto',\n",
    "    origin='lower',\n",
    "    extent=[0, T_total, -2, 2],\n",
    "    cmap='RdBu_r'\n",
    ")\n",
    "ax.axvline(T_history, color='yellow', linestyle='--', linewidth=2, label='Present (t=T)')\n",
    "ax.set_xlabel('Time Step')\n",
    "ax.set_ylabel('Spatial Position (x)')\n",
    "ax.set_title('Multi-Scale Wave System: Full Evolution')\n",
    "ax.legend()\n",
    "plt.colorbar(im, ax=ax, label='y(x,t)')\n",
    "\n",
    "# ==================================================\n",
    "# Top Right: Individual Components (FULL TIME RANGE)\n",
    "# ==================================================\n",
    "ax = axes[0, 1]\n",
    "ax.plot(components['slow_trend'][:, 50].numpy(), \n",
    "        label='Slow Trend (period ~100)', linewidth=2, color='blue')\n",
    "ax.plot(components['fast_wave'][:, 50].numpy(),  # Full range!\n",
    "        label='Fast Wave (period ~7)', linewidth=2, alpha=0.7, color='red')\n",
    "ax.set_xlabel('Time Step')\n",
    "ax.set_ylabel('Amplitude')\n",
    "ax.set_title('Individual Components at x=0 (FULL TIME RANGE)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0, T_total)\n",
    "\n",
    "# ==================================================\n",
    "# Bottom Left: Temporal Evolution at Selected Positions\n",
    "# ==================================================\n",
    "ax = axes[1, 0]\n",
    "ax.plot(y_sample[:, 25].numpy(), label='x=-1.0', alpha=0.7, linewidth=2)\n",
    "ax.plot(y_sample[:, 50].numpy(), label='x=0.0', alpha=0.7, linewidth=2)\n",
    "ax.plot(y_sample[:, 75].numpy(), label='x=1.0', alpha=0.7, linewidth=2)\n",
    "ax.axvline(T_history, color='red', linestyle='--', linewidth=2, label='Forecast Start')\n",
    "ax.set_xlabel('Time Step')\n",
    "ax.set_ylabel('y(x, t)')\n",
    "ax.set_title('Temporal Evolution at Selected Spatial Positions')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# ==================================================\n",
    "# Bottom Right: Frequency Spectrum (Shows Two Timescales!)\n",
    "# ==================================================\n",
    "ax = axes[1, 1]\n",
    "signal = y_sample[:, 50].numpy()\n",
    "n = len(signal)\n",
    "yf = fft(signal)\n",
    "xf = fftfreq(n, 1.0)[:n//2]\n",
    "power = 2.0/n * np.abs(yf[0:n//2])\n",
    "\n",
    "ax.plot(xf[1:], power[1:], linewidth=2, color='purple')\n",
    "ax.axvline(1/7.0, color='red', linestyle='--', label='Fast period ~7', linewidth=2)\n",
    "ax.axvline(1/100.0, color='blue', linestyle='--', label='Slow period ~100', linewidth=2)\n",
    "ax.set_xlabel('Frequency (1/steps)')\n",
    "ax.set_ylabel('Power')\n",
    "ax.set_title('Frequency Spectrum: Two Distinct Timescales')\n",
    "ax.set_xlim(0, 0.3)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('01_multiscale_data_overview.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n‚úì Visualization saved: 01_multiscale_data_overview.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Interpretation\n",
    "\n",
    "Look at the frequency spectrum (bottom right):\n",
    "- **Two clear peaks**: One at ~1/7 (fast wave), one at ~1/100 (slow trend)\n",
    "- **CNN limitation**: With 3 layers and kernel_size=7, receptive field = 19 steps\n",
    "  - Can see the fast wave (period 7) ‚úì\n",
    "  - Cannot see the slow trend (period 100) ‚úó\n",
    "- **Transformer advantage**: Global attention sees all 200 time steps!\n",
    "\n",
    "This is exactly why **Transformers dominate language modeling** - they can relate distant words that CNNs miss!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Training, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating datasets...\")\n",
    "print(\"This may take a minute...\\n\")\n",
    "\n",
    "# Training data\n",
    "x_train, y_history_train, y_future_train = dataset.generate_batch(\n",
    "    batch_size=n_train, \n",
    "    history_length=T_history, \n",
    "    forecast_horizon=H_forecast\n",
    ")\n",
    "print(f\"‚úì Training set: {y_history_train.shape} ‚Üí {y_future_train.shape}\")\n",
    "\n",
    "# Validation data\n",
    "x_val, y_history_val, y_future_val = dataset.generate_batch(\n",
    "    batch_size=n_val,\n",
    "    history_length=T_history,\n",
    "    forecast_horizon=H_forecast\n",
    ")\n",
    "print(f\"‚úì Validation set: {y_history_val.shape} ‚Üí {y_future_val.shape}\")\n",
    "\n",
    "# Test data\n",
    "x_test, y_history_test, y_future_test = dataset.generate_batch(\n",
    "    batch_size=n_test,\n",
    "    history_length=T_history,\n",
    "    forecast_horizon=H_forecast\n",
    ")\n",
    "print(f\"‚úì Test set: {y_history_test.shape} ‚Üí {y_future_test.shape}\")\n",
    "\n",
    "print(f\"\\nTotal samples: {n_train + n_val + n_test}\")\n",
    "print(f\"Data shapes: [Batch, Time, Space] = [B, {T_history}, {L_spatial}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Architecture Deep Dive\n",
    "\n",
    "Now let's build and understand each architecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Multi-Layer Perceptron (MLP)\n",
    "\n",
    "**Philosophy:** \"Treat everything as independent features\"\n",
    "\n",
    "**How it works:**\n",
    "```\n",
    "Input:  [B, 150, 100]\n",
    "   ‚Üì Flatten\n",
    "[B, 15,000] ‚Üê 15,000 independent features!\n",
    "   ‚Üì Fully connected layers\n",
    "[B, 512] ‚Üí [B, 256] ‚Üí [B, 128]\n",
    "   ‚Üì Output\n",
    "[B, 100] ‚Üê Next time step\n",
    "```\n",
    "\n",
    "**Strengths:**\n",
    "- Universal approximator (can learn any function)\n",
    "- Simple architecture\n",
    "\n",
    "**Weaknesses:**\n",
    "- Ignores temporal structure\n",
    "- Parameter explosion (millions of weights)\n",
    "- Position-specific learning (doesn't generalize across time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build MLP model\n",
    "mlp_ar_model = MultiLayerPerceptronAR(\n",
    "    history_length=T_history,\n",
    "    spatial_dim=L_spatial,\n",
    "    hidden_dims=[256, 128],  # Two hidden layers\n",
    "    activation='relu',\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "mlp_params = sum(p.numel() for p in mlp_ar_model.parameters())\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MLP ARCHITECTURE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total parameters: {mlp_params:,}\")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"  Input:  [B, {T_history}, {L_spatial}] ‚Üí Flatten ‚Üí [B, {T_history*L_spatial}]\")\n",
    "print(f\"  Hidden: [B, {T_history*L_spatial}] ‚Üí [B, 256] ‚Üí [B, 128]\")\n",
    "print(f\"  Output: [B, 128] ‚Üí [B, {L_spatial}] ‚Üí Reshape ‚Üí [B, 1, {L_spatial}]\")\n",
    "print(\"\\nKey feature: DESTROYS temporal structure by flattening!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Convolutional Neural Network (CNN)\n",
    "\n",
    "**Philosophy:** \"Detect reusable local patterns\"\n",
    "\n",
    "**How it works:**\n",
    "```\n",
    "Input:  [B, 150, 100]\n",
    "   ‚Üì Transpose\n",
    "[B, 100, 150] ‚Üê Treat 100 spatial dims as channels\n",
    "   ‚Üì 1D Convolution over time (kernel_size=7)\n",
    "[B, 32, 150] ‚Üí [B, 64, 150] ‚Üí [B, 32, 150]\n",
    "   ‚Üì Attention aggregation (learn which positions matter)\n",
    "[B, 32, 1]\n",
    "   ‚Üì Output projection\n",
    "[B, 1, 100] ‚Üê Next time step\n",
    "```\n",
    "\n",
    "**Strengths:**\n",
    "- Parameter sharing (same filter at all positions)\n",
    "- Captures local temporal patterns\n",
    "- Much fewer parameters than MLP\n",
    "\n",
    "**Weaknesses:**\n",
    "- Limited receptive field (only sees nearby time steps)\n",
    "- **CRITICAL**: BatchNorm breaks autoregressive models!\n",
    "  - We use `normalization='none'` to avoid this bug\n",
    "  - This is why GPT uses LayerNorm, not BatchNorm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN model (FIXED VERSION - no BatchNorm!)\n",
    "cnn_ar_model = CNN_AR_v2(\n",
    "    history_length=T_history,\n",
    "    spatial_dim=L_spatial,\n",
    "    channels=[32, 64, 32],        # 3 conv layers\n",
    "    kernel_sizes=[7, 7, 7],       # kernel=7 matches fast_wave period!\n",
    "    activation='relu',\n",
    "    dropout=0.1,\n",
    "    normalization='none',         # ‚Üê CRITICAL: No BatchNorm!\n",
    "    aggregation='attention'       # ‚Üê Learn which time positions matter\n",
    ").to(device)\n",
    "\n",
    "cnn_params = sum(p.numel() for p in cnn_ar_model.parameters())\n",
    "\n",
    "# Calculate receptive field\n",
    "n_layers = len([32, 64, 32])\n",
    "kernel_size = 7\n",
    "receptive_field = 1 + n_layers * (kernel_size - 1)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CNN ARCHITECTURE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total parameters: {cnn_params:,}\")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"  Input:  [B, {T_history}, {L_spatial}] ‚Üí Transpose ‚Üí [B, {L_spatial}, {T_history}]\")\n",
    "print(f\"  Conv1:  [B, {L_spatial}, {T_history}] ‚Üí [B, 32, {T_history}]  (kernel=7)\")\n",
    "print(f\"  Conv2:  [B, 32, {T_history}] ‚Üí [B, 64, {T_history}]  (kernel=7)\")\n",
    "print(f\"  Conv3:  [B, 64, {T_history}] ‚Üí [B, 32, {T_history}]  (kernel=7)\")\n",
    "print(f\"  Aggregate: [B, 32, {T_history}] ‚Üí [B, 32, 1]  (attention)\")\n",
    "print(f\"  Output: [B, 32, 1] ‚Üí [B, 1, {L_spatial}]\")\n",
    "print(f\"\\nReceptive field: {receptive_field} time steps\")\n",
    "print(f\"  - Can capture fast wave (period 7) ‚úì\")\n",
    "print(f\"  - Cannot capture slow trend (period 100) ‚úó\")\n",
    "print(f\"\\nKey features:\")\n",
    "print(f\"  - NO BatchNorm (critical for autoregressive!)\")\n",
    "print(f\"  - Kernel size = 7 matches fast wave period\")\n",
    "print(f\"  - Attention aggregation learns temporal importance\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Transformer\n",
    "\n",
    "**Philosophy:** \"Learn which relationships matter via attention\"\n",
    "\n",
    "**How it works:**\n",
    "```\n",
    "Input:  [B, 150, 100]\n",
    "   ‚Üì Project to embedding dimension\n",
    "[B, 150, 64] ‚Üê Each time step becomes 64-dim vector\n",
    "   ‚Üì Add positional encoding (sinusoidal)\n",
    "[B, 150, 64] with position information\n",
    "   ‚Üì Multi-head self-attention (4 heads, 2 layers)\n",
    "[B, 150, 64] ‚Üê Each position attends to ALL others\n",
    "   ‚Üì Take last position\n",
    "[B, 1, 64] ‚Üê This has \"seen\" all 150 time steps!\n",
    "   ‚Üì Project to output\n",
    "[B, 1, 100] ‚Üê Next time step\n",
    "```\n",
    "\n",
    "**Strengths:**\n",
    "- **Global receptive field** (sees all time steps)\n",
    "- Learns which past moments matter (attention weights)\n",
    "- Captures both short and long-range dependencies\n",
    "\n",
    "**Weaknesses:**\n",
    "- More parameters than CNN\n",
    "- O(T¬≤) complexity in sequence length\n",
    "- Needs more data to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Transformer model\n",
    "transformer_ar_model = TransformerAR(\n",
    "    history_length=T_history,\n",
    "    spatial_dim=L_spatial,\n",
    "    d_model=64,              # Embedding dimension\n",
    "    n_heads=4,               # Number of attention heads\n",
    "    n_layers=2,              # Number of transformer layers\n",
    "    dim_feedforward=128,     # FFN hidden dimension\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "transformer_params = sum(p.numel() for p in transformer_ar_model.parameters())\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRANSFORMER ARCHITECTURE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total parameters: {transformer_params:,}\")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"  Input projection: [B, {T_history}, {L_spatial}] ‚Üí [B, {T_history}, 64]\")\n",
    "print(f\"  Positional encoding: Add position information\")\n",
    "print(f\"  Transformer layers: 2 layers of multi-head attention (4 heads each)\")\n",
    "print(f\"    - Each position attends to ALL {T_history} positions\")\n",
    "print(f\"    - Learns which past moments are important\")\n",
    "print(f\"  Take last position: [B, {T_history}, 64] ‚Üí [B, 1, 64]\")\n",
    "print(f\"  Output projection: [B, 1, 64] ‚Üí [B, 1, {L_spatial}]\")\n",
    "print(f\"\\nReceptive field: INFINITE (global attention)\")\n",
    "print(f\"  - Captures fast wave (period 7) ‚úì\")\n",
    "print(f\"  - Captures slow trend (period 100) ‚úì\")\n",
    "print(f\"  - This is how GPT works!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Parameter Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PARAMETER COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<15} {'Parameters':<15} {'Relative Size'}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'MLP':<15} {mlp_params:>12,}   {mlp_params/mlp_params:>6.1f}x (baseline)\")\n",
    "print(f\"{'CNN':<15} {cnn_params:>12,}   {cnn_params/mlp_params:>6.1f}x\")\n",
    "print(f\"{'Transformer':<15} {transformer_params:>12,}   {transformer_params/mlp_params:>6.1f}x\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nNote: CNN has {mlp_params/cnn_params:.1f}x fewer parameters than MLP!\")\n",
    "print(f\"      This is the power of parameter sharing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Training Functions\n",
    "\n",
    "We'll use **teacher forcing** for training - this means during training, we feed the model the TRUE future values (not its own predictions). This is:\n",
    "- Much faster than autoregressive training\n",
    "- Standard practice for sequence models\n",
    "- What GPT uses during pre-training!\n",
    "\n",
    "At **test time**, we still use autoregressive generation (model sees its own predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoregressive(model, optimizer, y_history, y_future, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train one epoch using teacher forcing.\n",
    "    \n",
    "    Teacher forcing: At each step, we give the model the TRUE past,\n",
    "    not its own predictions. This is faster and more stable.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    n_batches = len(y_history) // batch_size\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        batch_history = y_history[start_idx:end_idx].to(device)\n",
    "        batch_future = y_future[start_idx:end_idx].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        \n",
    "        # Teacher forcing loop\n",
    "        for h in range(H_forecast):\n",
    "            if h == 0:\n",
    "                # First step: just use history\n",
    "                x_input = batch_history\n",
    "            else:\n",
    "                # Later steps: concatenate history with TRUE future (teacher forcing)\n",
    "                x_input = torch.cat([\n",
    "                    batch_history[:, h:, :],      # Shift history\n",
    "                    batch_future[:, :h, :]        # TRUE future (not predictions!)\n",
    "                ], dim=1)\n",
    "            \n",
    "            # Predict next step\n",
    "            y_pred = model(x_input)\n",
    "            \n",
    "            # Accumulate loss\n",
    "            loss = loss + nn.MSELoss()(y_pred[:, 0, :], batch_future[:, h, :])\n",
    "        \n",
    "        # Average loss over forecast horizon\n",
    "        loss = loss / H_forecast\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / n_batches\n",
    "\n",
    "\n",
    "def validate(model, y_history, y_future):\n",
    "    \"\"\"\n",
    "    Validate using AUTOREGRESSIVE generation.\n",
    "    \n",
    "    Important: At test time, we use autoregressive generation\n",
    "    (model sees its own predictions, not true values).\n",
    "    This is the true test of model quality!\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Autoregressive forecast (model.forecast uses its own predictions)\n",
    "        y_pred = model.forecast(y_history.to(device), n_steps=H_forecast)\n",
    "        \n",
    "        # Compute MSE\n",
    "        mse = nn.MSELoss()(y_pred, y_future.to(device)).item()\n",
    "    \n",
    "    return mse\n",
    "\n",
    "print(\"‚úì Training functions defined\")\n",
    "print(\"\\nKey difference:\")\n",
    "print(\"  - Training: Teacher forcing (fast, stable)\")\n",
    "print(\"  - Testing: Autoregressive (true performance test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Training All Models\n",
    "\n",
    "Now let's train all three models and compare their performance!\n",
    "\n",
    "**Note:** Training may take 10-20 minutes depending on your hardware. We use early stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of models and optimizers\n",
    "models = {\n",
    "    'MLP': (mlp_ar_model, optim.Adam(mlp_ar_model.parameters(), lr=learning_rate)),\n",
    "    'CNN': (cnn_ar_model, optim.Adam(cnn_ar_model.parameters(), lr=learning_rate)),\n",
    "    'Transformer': (transformer_ar_model, optim.Adam(transformer_ar_model.parameters(), lr=learning_rate))\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING ALL MODELS\")\n",
    "print(\"=\"*70)\n",
    "print(\"This may take 10-20 minutes...\\n\")\n",
    "\n",
    "for name, (model, optimizer) in models.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training {name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    best_val_mse = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 5  # Early stopping patience\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Train\n",
    "        train_loss = train_autoregressive(\n",
    "            model, optimizer, \n",
    "            y_history_train, y_future_train,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_mse = validate(model, y_history_val, y_future_val)\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if val_mse < best_val_mse:\n",
    "            best_val_mse = val_mse\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Log progress\n",
    "        if epoch % 3 == 0:\n",
    "            print(f\"Epoch {epoch:2d}/{n_epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.6f} | \"\n",
    "                  f\"Val MSE: {val_mse:.6f} | \"\n",
    "                  f\"Best: {best_val_mse:.6f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\n‚úì Early stopping at epoch {epoch}\")\n",
    "            print(f\"  Best validation MSE: {best_val_mse:.6f}\")\n",
    "            break\n",
    "    \n",
    "    # Test\n",
    "    test_mse = validate(model, y_history_test, y_future_test)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'test_mse': test_mse,\n",
    "        'best_val_mse': best_val_mse\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úì {name} Final Test MSE: {test_mse:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL MODELS TRAINED!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Results Analysis\n",
    "\n",
    "Let's compare the models and see which architecture performs best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort results by test MSE\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1]['test_mse'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Rank':<6} {'Model':<15} {'Test MSE':<12} {'Parameters':<15} {'Status'}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for rank, (name, result) in enumerate(sorted_results, 1):\n",
    "    params = {\n",
    "        'MLP': mlp_params,\n",
    "        'CNN': cnn_params,\n",
    "        'Transformer': transformer_params\n",
    "    }[name]\n",
    "    \n",
    "    status = \"üèÜ\" if rank == 1 else (\"ü•à\" if rank == 2 else \"ü•â\")\n",
    "    \n",
    "    print(f\"#{rank:<5} {name:<15} {result['test_mse']:<12.6f} {params:>12,}   {status}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Analysis\n",
    "mlp_mse = results['MLP']['test_mse']\n",
    "cnn_mse = results['CNN']['test_mse']\n",
    "transformer_mse = results['Transformer']['test_mse']\n",
    "\n",
    "print(\"\\nüìä ANALYSIS:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "if transformer_mse < cnn_mse < mlp_mse:\n",
    "    print(\"‚úÖ Perfect hierarchy: Transformer < CNN < MLP\")\n",
    "    print(\"\\nThis demonstrates:\")\n",
    "    print(\"  - MLP struggles with temporal structure\")\n",
    "    print(\"  - CNN captures local patterns (fast wave)\")\n",
    "    print(\"  - Transformer captures all scales (global attention)\")\n",
    "elif transformer_mse < min(cnn_mse, mlp_mse):\n",
    "    print(\"‚úÖ Transformer wins! (Main lesson achieved)\")\n",
    "    print(f\"\\nOrder: {sorted_results[0][0]} < {sorted_results[1][0]} < {sorted_results[2][0]}\")\n",
    "    print(\"\\nKey insight: Global attention captures all timescales!\")\n",
    "else:\n",
    "    print(f\"Results: {sorted_results[0][0]} < {sorted_results[1][0]} < {sorted_results[2][0]}\")\n",
    "    print(\"\\nNote: Relative performance may vary with random seed.\")\n",
    "    print(\"Key lesson: Architecture choice matters for data structure!\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Detailed Error Analysis\n",
    "\n",
    "Let's visualize predictions and analyze errors in detail!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for one test sample\n",
    "test_idx = 0\n",
    "y_history_sample = y_history_test[test_idx:test_idx+1].to(device)\n",
    "y_future_sample = y_future_test[test_idx]\n",
    "\n",
    "# Generate predictions from all models\n",
    "predictions = {}\n",
    "for name, result in results.items():\n",
    "    model = result['model']\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model.forecast(y_history_sample, n_steps=H_forecast).cpu()[0]\n",
    "        predictions[name] = pred\n",
    "\n",
    "print(\"‚úì Generated predictions from all models\")\n",
    "print(f\"  Prediction shape: {predictions['MLP'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Comprehensive 9-Panel Comparison\n",
    "\n",
    "We'll create three types of plots for each model:\n",
    "1. **Time series predictions** - How well do predictions match truth?\n",
    "2. **Error evolution** - Where do errors occur?\n",
    "3. **MSE accumulation** - Do errors compound over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3x3 grid (3 models √ó 3 plot types)\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "pos = 50  # x=0 position\n",
    "\n",
    "for i, (name, y_pred) in enumerate(predictions.items()):\n",
    "    \n",
    "    # ========================================\n",
    "    # ROW 1: Time Series Predictions\n",
    "    # ========================================\n",
    "    ax = fig.add_subplot(gs[0, i])\n",
    "    \n",
    "    # History (black line)\n",
    "    ax.plot(range(T_history), \n",
    "            y_history_test[test_idx, :, pos].numpy(),\n",
    "            'k-', linewidth=2, label='History', alpha=0.7)\n",
    "    \n",
    "    # True future (blue dashed)\n",
    "    ax.plot(range(T_history, T_history + H_forecast),\n",
    "            y_future_sample[:, pos].numpy(),\n",
    "            'b--', linewidth=2, label='True Future')\n",
    "    \n",
    "    # Prediction (red with markers)\n",
    "    ax.plot(range(T_history, T_history + H_forecast),\n",
    "            y_pred[:, pos].numpy(),\n",
    "            'r-', marker='o', markersize=4, linewidth=2, label='Prediction')\n",
    "    \n",
    "    ax.axvline(T_history, color='green', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Time Step', fontsize=10)\n",
    "    ax.set_ylabel('y(x=0, t)', fontsize=10)\n",
    "    ax.set_title(f'{name}\\nTest MSE: {results[name][\"test_mse\"]:.4f}', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=8, loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ========================================\n",
    "    # ROW 2: Error Evolution\n",
    "    # ========================================\n",
    "    ax = fig.add_subplot(gs[1, i])\n",
    "    \n",
    "    errors = (y_pred[:, pos] - y_future_sample[:, pos]).numpy()\n",
    "    time_steps = range(H_forecast)\n",
    "    \n",
    "    ax.plot(time_steps, errors, 'r-', linewidth=2, label='Error')\n",
    "    ax.axhline(0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax.fill_between(time_steps, errors, 0, alpha=0.3, color='red')\n",
    "    \n",
    "    ax.set_xlabel('Forecast Step', fontsize=10)\n",
    "    ax.set_ylabel('Prediction Error', fontsize=10)\n",
    "    ax.set_title(f'{name}: Error Evolution', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=8)\n",
    "    \n",
    "    # ========================================\n",
    "    # ROW 3: MSE Accumulation with Confidence\n",
    "    # ========================================\n",
    "    ax = fig.add_subplot(gs[2, i])\n",
    "    \n",
    "    # Compute pointwise squared errors (averaged over spatial dimension)\n",
    "    squared_errors = (y_pred - y_future_sample).pow(2).mean(dim=1).numpy()\n",
    "    \n",
    "    # Plot MSE per step\n",
    "    ax.plot(time_steps, squared_errors, 'b-', linewidth=2, marker='o', \n",
    "            markersize=6, label='MSE per step')\n",
    "    \n",
    "    # Mean MSE line\n",
    "    mean_mse = squared_errors.mean()\n",
    "    ax.axhline(mean_mse, color='r', linestyle='--', linewidth=2, \n",
    "               label=f'Mean: {mean_mse:.4f}')\n",
    "    \n",
    "    # Confidence band (¬±1 std)\n",
    "    std_error = squared_errors.std()\n",
    "    ax.fill_between(time_steps, \n",
    "                     mean_mse - std_error, \n",
    "                     mean_mse + std_error,\n",
    "                     alpha=0.2, color='red', label=f'¬±1œÉ: {std_error:.4f}')\n",
    "    \n",
    "    ax.set_xlabel('Forecast Step', fontsize=10)\n",
    "    ax.set_ylabel('Mean Squared Error', fontsize=10)\n",
    "    ax.set_title(f'{name}: MSE Accumulation', fontsize=11)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(bottom=0)\n",
    "\n",
    "plt.suptitle('Architecture Comparison: Predictions & Error Analysis', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.savefig('multiscale_comparison_with_errors.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n‚úì Visualization saved: multiscale_comparison_with_errors.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Interpreting the Results\n",
    "\n",
    "**Row 1 (Predictions):**\n",
    "- Look at how well each model tracks the oscillations\n",
    "- Does the prediction match the frequency of the true signal?\n",
    "\n",
    "**Row 2 (Error Evolution):**\n",
    "- **MLP**: Large, random errors (can't capture structure)\n",
    "- **CNN**: Smaller errors with patterns (captures fast wave)\n",
    "- **Transformer**: Smallest, most random errors (captures everything)\n",
    "\n",
    "**Row 3 (MSE Accumulation):**\n",
    "- Flat line = stable predictions\n",
    "- Increasing line = error accumulation (autoregressive drift)\n",
    "- Narrow confidence band = consistent performance\n",
    "- Wide confidence band = unreliable predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Architecture choice matters!**\n",
    "   - MLP: Universal but inefficient (no inductive bias)\n",
    "   - CNN: Great for local patterns (parameter sharing)\n",
    "   - Transformer: Best for long-range dependencies (global attention)\n",
    "\n",
    "2. **Receptive field is critical:**\n",
    "   - CNN receptive field = 19 steps (can't see 100-step slow trend)\n",
    "   - Transformer receptive field = ‚àû (sees everything)\n",
    "\n",
    "3. **BatchNorm breaks autoregressive models:**\n",
    "   - Training sees TRUE data, generation sees PREDICTIONS\n",
    "   - Distribution mismatch ‚Üí feature collapse\n",
    "   - This is why GPT uses LayerNorm!\n",
    "\n",
    "4. **Multi-scale patterns reveal architecture limits:**\n",
    "   - Fast patterns (period 7) ‚Üí CNN excels\n",
    "   - Slow patterns (period 100) ‚Üí Transformer needed\n",
    "   - Real-world data has multiple scales!\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "- **Weather forecasting**: Fast oscillations + seasonal trends\n",
    "- **Stock market**: High-frequency + macro trends\n",
    "- **Language modeling**: Local context + long-range dependencies (GPT!)\n",
    "- **Video prediction**: Frame-to-frame + scene-level patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Experiments to Try\n",
    "\n",
    "Now it's your turn to experiment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Adjust Data Components\n",
    "\n",
    "Try changing the amplitude of slow vs fast components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Emphasize Transformer advantage\n",
    "dataset_slow_heavy = MultiScaleWaveDataset(\n",
    "    T=T_total, L=L_spatial, x_range=x_range,\n",
    "    slow_amplitude=0.8,   # Increase slow trend\n",
    "    fast_amplitude=0.3,   # Decrease fast wave\n",
    "    seed=SEED + 1\n",
    ")\n",
    "\n",
    "# Expected: Transformer advantage increases, CNN struggles more\n",
    "print(\"Try retraining models on this dataset!\")\n",
    "print(\"Expected: Larger gap between Transformer and CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Change Model Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Bigger CNN - can it compete with Transformer?\n",
    "cnn_large = CNN_AR_v2(\n",
    "    history_length=T_history, spatial_dim=L_spatial,\n",
    "    channels=[64, 128, 64],      # Bigger (was [32, 64, 32])\n",
    "    kernel_sizes=[11, 11, 11],   # Larger kernels (was [7, 7, 7])\n",
    "    normalization='none',\n",
    "    aggregation='attention'\n",
    ").to(device)\n",
    "\n",
    "# Receptive field calculation\n",
    "rf_large = 1 + 3 * (11 - 1)\n",
    "print(f\"Large CNN receptive field: {rf_large} steps (was 19)\")\n",
    "print(\"\\nStill can't see 100-step slow trend!\")\n",
    "print(\"Try training and comparing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Compare Aggregation Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Different aggregation methods\n",
    "aggregations = ['adaptive_avg', 'last_position', 'attention']\n",
    "\n",
    "print(\"Try these CNN aggregation strategies:\")\n",
    "for agg in aggregations:\n",
    "    print(f\"\\n  aggregation='{agg}'\")\n",
    "    if agg == 'adaptive_avg':\n",
    "        print(\"    - Averages all time positions (loses temporal dynamics)\")\n",
    "    elif agg == 'last_position':\n",
    "        print(\"    - Takes only t=149 (most recent, but padding contamination)\")\n",
    "    else:\n",
    "        print(\"    - Learns which positions matter (best, but more parameters)\")\n",
    "\n",
    "print(\"\\nWhich works best? Try it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've completed Tutorial 3 üéâ\n",
    "\n",
    "### What You've Achieved:\n",
    "\n",
    "‚úÖ Implemented three different architectures for time-series\n",
    "‚úÖ Understood how each architecture processes sequential data\n",
    "‚úÖ Discovered architecture-task matching principles\n",
    "‚úÖ Learned about receptive fields and their importance\n",
    "‚úÖ Found a critical bug: BatchNorm + Autoregressive\n",
    "‚úÖ Analyzed predictions with error evolution and confidence bands\n",
    "\n",
    "### The Big Picture:\n",
    "\n",
    "```\n",
    "MLP ‚Üí CNN ‚Üí Transformer\n",
    "\n",
    "No structure ‚Üí Local structure ‚Üí Global structure\n",
    "exploitation     exploitation      exploitation\n",
    "\n",
    "This is the evolution of deep learning!\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Tutorial 4**: Denoising Diffusion Models (DDPM)\n",
    "- **Tutorial 5**: Flow Matching\n",
    "- Try these models on real-world data!\n",
    "- Implement your own hybrid architecture\n",
    "\n",
    "---\n",
    "\n",
    "**Questions? Issues?**\n",
    "- Check the FAQ\n",
    "- See Troubleshooting guide\n",
    "- Open an issue on GitHub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
