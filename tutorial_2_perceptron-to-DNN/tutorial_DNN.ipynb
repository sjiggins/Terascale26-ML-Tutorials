{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: From Perceptrons to Deep Neural Networks\n",
    "\n",
    "## Welcome!\n",
    "\n",
    "In this tutorial, you'll learn the fundamental challenges and solutions for training deep neural networks.\n",
    "\n",
    "**What you'll master:**\n",
    "- The vanishing gradient problem\n",
    "- Activation functions (sigmoid, tanh, ReLU)\n",
    "- Detecting overfitting\n",
    "- Regularization techniques (L1, L2, Dropout)\n",
    "\n",
    "**Time commitment:** 30-45mins\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic Python and PyTorch\n",
    "- Understanding of neural networks\n",
    "- Completed Tutorial 1 (recommended)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import all the necessary modules from our tutorial package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "PyTorch version: 2.10.0+cpu\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Tutorial modules\n",
    "from perceptron_to_DNN_tutorial.MultiLayerPerceptron import MultiLayerPerceptron\n",
    "from perceptron_to_DNN_tutorial.train import (\n",
    "    train_model_with_gradient_tracking,\n",
    "    train_model_with_validation_tracking\n",
    ")\n",
    "from perceptron_to_DNN_tutorial.utils import (\n",
    "    generate_high_order_polynomial_data,\n",
    "    FeatureNormalizer\n",
    ")\n",
    "from perceptron_to_DNN_tutorial.plotting import (\n",
    "    plot_gradient_flow,\n",
    "    plot_layer_gradient_norms,\n",
    "    plot_regularization_comparison,\n",
    "    plot_results\n",
    ")\n",
    "\n",
    "# Initialize logger\n",
    "import logging\n",
    "from perceptron_to_DNN_tutorial.logger import configure_logging\n",
    "configure_logging()\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Let's set up our experimental parameters. These control:\n",
    "- Network architecture (depth and width)\n",
    "- Data properties (polynomial complexity, noise)\n",
    "- Training hyperparameters\n",
    "\n",
    "**Note:** Feel free to modify these later to see how results change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " CONFIGURATION\n",
      "======================================================================\n",
      "Polynomial order: 9\n",
      "True coefficients: [10.0, 0.5, -0.04, 0.015, -0.001, -0.0003, 5.5e-05, -5e-06, -1e-07, 2e-08]\n",
      "Training samples: 200\n",
      "Validation samples: 200\n",
      "Test samples: 200\n",
      "Network architecture: 1 → 128 → 128 → 128 → 128 → 128 → 128 → 128 → 128 → 128 → 1\n",
      "Activations to test: ['sigmoid', 'tanh', 'relu']\n",
      "Training epochs (gradient demo): 10000\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CONFIGURATION\n",
    "# ========================================\n",
    "\n",
    "# Network Architecture\n",
    "# This is a DEEP network: 10 layers total (1 input → 9 hidden → 1 output)\n",
    "architecture_deep = [1, 128, 128, 128, 128, 128, 128, 128, 128, 128, 1]\n",
    "\n",
    "# Data Generation Parameters\n",
    "poly_order = 9           # High-order polynomial (complex function)\n",
    "coeffs_true = [10.0, 0.5, -0.04, 0.015, -0.001, -0.0003, 0.000055, -0.000005, -1e-7, 2e-8]\n",
    "n_samples_train = 200    # Training set size\n",
    "n_samples_valid = 200    # Validation set size  \n",
    "n_samples_test = 200     # Test set size\n",
    "noise_std = 2.5          # Noise level in data\n",
    "x_min = 0                # Input range start\n",
    "x_max = 10               # Input range end\n",
    "\n",
    "# Training Hyperparameters\n",
    "num_epochs = 10000       # Training iterations (for vanishing gradient demo)\n",
    "learning_rate = 0.005    # Step size for gradient descent\n",
    "\n",
    "# Activation functions to test\n",
    "activations_to_test = ['sigmoid', 'tanh', 'relu']\n",
    "\n",
    "# Regularization parameters\n",
    "lambda_l2 = 0.01         # L2 regularization strength\n",
    "lambda_l1 = 0.001        # L1 regularization strength\n",
    "dropout_rate = 0.1       # Dropout probability\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Polynomial order: {poly_order}\")\n",
    "print(f\"True coefficients: {coeffs_true}\")\n",
    "print(f\"Training samples: {n_samples_train}\")\n",
    "print(f\"Validation samples: {n_samples_valid}\")\n",
    "print(f\"Test samples: {n_samples_test}\")\n",
    "print(f\"Network architecture: {' → '.join(map(str, architecture_deep))}\")\n",
    "print(f\"Activations to test: {activations_to_test}\")\n",
    "print(f\"Training epochs (gradient demo): {num_epochs}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: The Vanishing Gradient Problem\n",
    "\n",
    "## What is it?\n",
    "\n",
    "In deep networks, gradients can shrink exponentially as they backpropagate through layers:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial h_9} \\cdot \\frac{\\partial h_9}{\\partial h_8} \\cdot \\ldots \\cdot \\frac{\\partial h_2}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial W_1}$$\n",
    "\n",
    "With sigmoid/tanh, each derivative can be < 1, so the product **vanishes** (approaches 0).\n",
    "\n",
    "## Why does it matter?\n",
    "\n",
    "- Early layers get **tiny gradients** → learn very slowly\n",
    "- Network becomes effectively **shallow**\n",
    "- Can't leverage the power of depth\n",
    "\n",
    "## What's the solution?\n",
    "\n",
    "**ReLU activation!** ReLU'(x) = 1 for x > 0, preventing gradient decay.\n",
    "\n",
    "Let's see this in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1: Generate Datasets\n",
    "\n",
    "We'll create three separate datasets:\n",
    "- **Training**: Used to update weights\n",
    "- **Validation**: Used to monitor overfitting\n",
    "- **Test**: Used for final evaluation\n",
    "\n",
    "**Key principle:** Use different random seeds so datasets are truly independent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " PART 1: VANISHING GRADIENT PROBLEM\n",
      "======================================================================\n",
      "\n",
      "[STEP 1.1] Generating train/validation/test datasets...\n",
      "✓ Training: 200 samples, y range [-2.47, 17.12]\n",
      "✓ Validation: 200 samples, y range [-0.88, 17.32]\n",
      "✓ Test: 200 samples, y range [-2.13, 17.39]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" PART 1: VANISHING GRADIENT PROBLEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[STEP 1.1] Generating train/validation/test datasets...\")\n",
    "\n",
    "# Generate training data\n",
    "x_train, y_train = generate_high_order_polynomial_data(\n",
    "    coeffs_true=coeffs_true,\n",
    "    poly_order=poly_order,\n",
    "    n_samples=n_samples_train,\n",
    "    x_min=x_min,\n",
    "    x_max=x_max,\n",
    "    noise_std=noise_std,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Generate validation data (different seed!)\n",
    "x_valid, y_valid = generate_high_order_polynomial_data(\n",
    "    coeffs_true=coeffs_true,\n",
    "    poly_order=poly_order,\n",
    "    n_samples=n_samples_valid,\n",
    "    x_min=x_min,\n",
    "    x_max=x_max,\n",
    "    noise_std=noise_std,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "# Generate test data (different seed!)\n",
    "x_test, y_test = generate_high_order_polynomial_data(\n",
    "    coeffs_true=coeffs_true,\n",
    "    poly_order=poly_order,\n",
    "    n_samples=n_samples_test,\n",
    "    x_min=x_min,\n",
    "    x_max=x_max,\n",
    "    noise_std=noise_std,\n",
    "    seed=456\n",
    ")\n",
    "\n",
    "print(f\"✓ Training: {len(x_train)} samples, y range [{y_train.min():.2f}, {y_train.max():.2f}]\")\n",
    "print(f\"✓ Validation: {len(x_valid)} samples, y range [{y_valid.min():.2f}, {y_valid.max():.2f}]\")\n",
    "print(f\"✓ Test: {len(x_test)} samples, y range [{y_test.min():.2f}, {y_test.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2: Normalize Features\n",
    "\n",
    "Neural networks work best with normalized inputs. We'll normalize to the range [-1, 1].\n",
    "\n",
    "**Critical:** Use training statistics to normalize ALL datasets (train, val, test)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1.2] Normalizing features...\n",
      "2026-02-25 12:15:37,266 | INFO     | perceptron_to_DNN_tutorial.utils:233 | Fitted symmetric normalizer: mean=None, std=None, min=0.0, max=10.0\n",
      "✓ Features normalized to [-1.00, 1.00] using training statistics\n",
      "  (Validation and test use same normalization as training)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 1.2] Normalizing features...\")\n",
    "\n",
    "# Fit normalizer on training data only\n",
    "normalizer = FeatureNormalizer(method='symmetric')\n",
    "x_t = normalizer.fit_transform(x_train)\n",
    "y_t = y_train\n",
    "\n",
    "# Apply same normalization to validation and test\n",
    "x_valid_norm = normalizer.transform(x_valid)\n",
    "x_test_norm = normalizer.transform(x_test)\n",
    "\n",
    "print(f\"✓ Features normalized to [{x_t.min():.2f}, {x_t.max():.2f}] using training statistics\")\n",
    "print(f\"  (Validation and test use same normalization as training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.3: Train with Different Activations\n",
    "\n",
    "Now we'll train **deep networks** (9 hidden layers) with three activation functions:\n",
    "\n",
    "1. **Sigmoid**: sigmoid(x) = 1/(1+e^(-x)), derivative ∈ (0, 0.25]\n",
    "2. **Tanh**: tanh(x), derivative ∈ (0, 1]\n",
    "3. **ReLU**: max(0, x), derivative = 1 for x > 0\n",
    "\n",
    "We'll track gradients at each layer to see the vanishing gradient problem in action!\n",
    "\n",
    "**This will take a few minutes** ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1.3] Training deep networks with different activations...\n",
      "This demonstrates the vanishing gradient problem!\n",
      "\n",
      "\n",
      "--- Training with SIGMOID activation ---\n",
      "2026-02-25 12:15:37,284 | INFO     | perceptron_to_DNN_tutorial.MultiLayerPerceptron:242 | Initialized MultiLayerPerceptron:\n",
      "2026-02-25 12:15:37,285 | INFO     | perceptron_to_DNN_tutorial.MultiLayerPerceptron:243 |   Architecture: 1 → 128 → 128 → 128 → 128 → 128 → 128 → 128 → 128 → 128 → 1\n",
      "2026-02-25 12:15:37,285 | INFO     | perceptron_to_DNN_tutorial.MultiLayerPerceptron:244 |   Activation: sigmoid\n",
      "2026-02-25 12:15:37,286 | INFO     | perceptron_to_DNN_tutorial.MultiLayerPerceptron:245 |   Dropout rate: 0.0\n",
      "2026-02-25 12:15:37,287 | INFO     | perceptron_to_DNN_tutorial.MultiLayerPerceptron:246 |   Total parameters: 132481\n",
      "2026-02-25 12:15:37,287 | INFO     | perceptron_to_DNN_tutorial.MultiLayerPerceptron:247 |   Hidden layers: 9\n",
      "2026-02-25 12:15:39,901 | INFO     | perceptron_to_DNN_tutorial.train:494 | ======================================================================\n",
      "2026-02-25 12:15:39,902 | INFO     | perceptron_to_DNN_tutorial.train:495 | Training with Gradient Tracking + Validation\n",
      "2026-02-25 12:15:39,903 | INFO     | perceptron_to_DNN_tutorial.train:497 | ======================================================================\n",
      "2026-02-25 12:15:39,903 | INFO     | perceptron_to_DNN_tutorial.train:498 | Model: MultiLayerPerceptron\n",
      "2026-02-25 12:15:39,904 | INFO     | perceptron_to_DNN_tutorial.train:500 | Architecture: 1 → 128 → 128 → 128 → 128 → 128 → 128 → 128 → 128 → 128 → 1\n",
      "2026-02-25 12:15:39,904 | INFO     | perceptron_to_DNN_tutorial.train:501 | Activation: sigmoid\n",
      "2026-02-25 12:15:39,905 | INFO     | perceptron_to_DNN_tutorial.train:504 | Regularization: none (λ₁=0.0, λ₂=0.0)\n",
      "2026-02-25 12:15:39,905 | INFO     | perceptron_to_DNN_tutorial.train:505 | Learning rate: 0.005\n",
      "2026-02-25 12:15:39,906 | INFO     | perceptron_to_DNN_tutorial.train:506 | Epochs: 10000\n",
      "2026-02-25 12:15:39,906 | INFO     | perceptron_to_DNN_tutorial.train:507 | Training samples: 200\n",
      "2026-02-25 12:15:39,907 | INFO     | perceptron_to_DNN_tutorial.train:509 | Validation samples: 200\n",
      "2026-02-25 12:15:39,907 | INFO     | perceptron_to_DNN_tutorial.train:510 | ======================================================================\n",
      "2026-02-25 12:15:43,057 | INFO     | perceptron_to_DNN_tutorial.train:604 | Epoch [ 500/10000] | Train Loss: 13.516261 | Valid Loss: 11.521521 | Gap: -1.994740\n",
      "2026-02-25 12:15:46,256 | INFO     | perceptron_to_DNN_tutorial.train:604 | Epoch [1000/10000] | Train Loss: 13.516265 | Valid Loss: 11.521523 | Gap: -1.994741\n",
      "2026-02-25 12:15:47,903 | INFO     | perceptron_to_DNN_tutorial.train:604 | Epoch [1500/10000] | Train Loss: 13.516265 | Valid Loss: 11.521523 | Gap: -1.994741\n",
      "2026-02-25 12:15:50,082 | INFO     | perceptron_to_DNN_tutorial.train:604 | Epoch [2000/10000] | Train Loss: 13.516265 | Valid Loss: 11.521523 | Gap: -1.994741\n",
      "2026-02-25 12:15:52,873 | INFO     | perceptron_to_DNN_tutorial.train:604 | Epoch [2500/10000] | Train Loss: 13.516261 | Valid Loss: 11.521521 | Gap: -1.994740\n",
      "2026-02-25 12:15:54,928 | INFO     | perceptron_to_DNN_tutorial.train:604 | Epoch [3000/10000] | Train Loss: 13.516262 | Valid Loss: 11.521523 | Gap: -1.994740\n",
      "2026-02-25 12:15:56,797 | INFO     | perceptron_to_DNN_tutorial.train:604 | Epoch [3500/10000] | Train Loss: 13.516262 | Valid Loss: 11.521523 | Gap: -1.994740\n",
      "2026-02-25 12:15:58,545 | INFO     | perceptron_to_DNN_tutorial.train:604 | Epoch [4000/10000] | Train Loss: 13.516262 | Valid Loss: 11.521523 | Gap: -1.994740\n",
      "2026-02-25 12:16:00,416 | INFO     | perceptron_to_DNN_tutorial.train:604 | Epoch [4500/10000] | Train Loss: 13.516265 | Valid Loss: 11.521523 | Gap: -1.994742\n",
      "2026-02-25 12:16:02,337 | INFO     | perceptron_to_DNN_tutorial.train:604 | Epoch [5000/10000] | Train Loss: 13.516265 | Valid Loss: 11.521523 | Gap: -1.994741\n",
      "2026-02-25 12:16:04,119 | INFO     | perceptron_to_DNN_tutorial.train:604 | Epoch [5500/10000] | Train Loss: 13.516262 | Valid Loss: 11.521523 | Gap: -1.994740\n",
      "2026-02-25 12:16:06,078 | INFO     | perceptron_to_DNN_tutorial.train:604 | Epoch [6000/10000] | Train Loss: 13.516262 | Valid Loss: 11.521523 | Gap: -1.994740\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 1.3] Training deep networks with different activations...\")\n",
    "print(\"This demonstrates the vanishing gradient problem!\\n\")\n",
    "\n",
    "# Store training histories\n",
    "gradient_histories = {}\n",
    "\n",
    "for activation in activations_to_test:\n",
    "    print(f\"\\n--- Training with {activation.upper()} activation ---\")\n",
    "    \n",
    "    # Create model\n",
    "    model = MultiLayerPerceptron(\n",
    "        layer_sizes=architecture_deep,\n",
    "        activation=activation,\n",
    "    )\n",
    "    \n",
    "    # Train with gradient tracking and per-sample distributions\n",
    "    trained_model, history = train_model_with_gradient_tracking(\n",
    "        model=model,\n",
    "        x_train=x_t,\n",
    "        y_train=y_t,\n",
    "        x_valid=x_valid_norm,\n",
    "        y_valid=y_valid,\n",
    "        num_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        reg_type='none',  # No regularization\n",
    "        print_every=500,\n",
    "        verbose=True,\n",
    "        track_gradients=True,\n",
    "        track_per_sample_gradients=False  # For distribution plots\n",
    "    )\n",
    "    \n",
    "    # Store full history (includes per-sample gradients)\n",
    "    gradient_histories[activation] = history\n",
    "    \n",
    "    # Analyze final gradients\n",
    "    final_grads = history['gradient_norms'][-1]\n",
    "    layer_names = [name for name in final_grads.keys() if 'weight' in name]\n",
    "    \n",
    "    print(f\"\\nFinal gradient magnitudes ({activation}):\")\n",
    "    for layer_name in layer_names:\n",
    "        print(f\"  {layer_name}: {final_grads[layer_name]:.6e}\")\n",
    "    \n",
    "    # Check for vanishing\n",
    "    first_layer_grad = final_grads[layer_names[0]]\n",
    "    last_layer_grad = final_grads[layer_names[-1]]\n",
    "    if first_layer_grad > 0:\n",
    "        ratio = last_layer_grad / first_layer_grad\n",
    "        print(f\"\\nGradient ratio (last/first layer): {ratio:.6e}\")\n",
    "        if ratio < 0.01:\n",
    "            print(\"WARNING: VANISHING GRADIENT DETECTED!\")\n",
    "        else:\n",
    "            print(\"✓ Gradients flowing reasonably through all layers\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training complete! Now let's visualize the results...\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.4: Visualize Gradient Flow\n",
    "\n",
    "This plot shows how gradient magnitudes change over training for each layer.\n",
    "\n",
    "**What to look for:**\n",
    "- **Sigmoid**: Gradients in early layers → 0 (exponential decay)\n",
    "- **Tanh**: Moderate gradient decay\n",
    "- **ReLU**: Stable gradients across all layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[STEP 1.4] Visualizing gradient flow...\")\n",
    "\n",
    "example_model = MultiLayerPerceptron(architecture_deep, activation='relu')\n",
    "plot_gradient_flow(\n",
    "    gradient_histories, \n",
    "    example_model, \n",
    "    activations_to_test,\n",
    "    save_name='vanishing_gradient_demo.png'\n",
    ")\n",
    "\n",
    "print(\"\\n  Key Observation:\")\n",
    "print(\"   - Sigmoid: Gradients vanish in early layers (red warning box)\")\n",
    "print(\"   - Tanh: Moderate gradient flow\")\n",
    "print(\"   - ReLU: Healthy gradient flow (green box)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.5: Visualize Gradient Distributions\n",
    "\n",
    "These plots show the **distribution** of signed gradient values across all training samples.\n",
    "\n",
    "We'll create **3 separate plots** for first, middle, and last epochs.\n",
    "\n",
    "**What to look for:**\n",
    "- **Healthy**: Wide distribution symmetric around zero\n",
    "- **Vanishing**: Distribution collapsed near zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[STEP 1.5] Visualizing gradient norm distributions across epochs...\")\n",
    "print(\"Creating 3 separate plots: first epoch, middle epoch, last epoch\\n\")\n",
    "\n",
    "plot_layer_gradient_norms(\n",
    "    gradient_histories,\n",
    "    example_model,\n",
    "    activations_to_test,\n",
    "    save_name='gradient_distributions',\n",
    "    signed=False,\n",
    "    remove_zero_gradients=False,\n",
    ")\n",
    "\n",
    "print(\"\\n Key Observations:\")\n",
    "print(\"   First epoch: All activations show reasonable gradients\")\n",
    "print(\"   Last epoch: Sigmoid collapsed to ~0, ReLU still healthy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.6: Visualize Best Model (ReLU)\n",
    "\n",
    "Let's see how well the ReLU model fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[STEP 1.6] Visualizing ReLU model fit...\")\n",
    "\n",
    "# Train a fresh ReLU model for visualization\n",
    "relu_model = MultiLayerPerceptron(architecture_deep, activation='relu', dropout_rate=0.0)\n",
    "relu_model, relu_history = train_model_with_gradient_tracking(\n",
    "    model=relu_model,\n",
    "    x_train=x_t,\n",
    "    y_train=y_t,\n",
    "    x_valid=x_valid_norm,\n",
    "    y_valid=y_valid,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    reg_type='none',\n",
    "    print_every=num_epochs + 1,  # Silent\n",
    "    track_gradients=False  # Faster\n",
    ")\n",
    "\n",
    "# Visualize results\n",
    "plot_results(\n",
    "    x_t, y_t,\n",
    "    relu_model,\n",
    "    relu_history,\n",
    "    coeffs_true,\n",
    "    poly_order,\n",
    "    model_name=\"Deep MLP with ReLU\",\n",
    "    normalizer=normalizer,\n",
    "    show_validation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Summary\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "1. **Sigmoid and Tanh suffer from vanishing gradients** in deep networks\n",
    "   - Gradients shrink exponentially through layers\n",
    "   - Early layers barely learn\n",
    "\n",
    "2. **ReLU solves the vanishing gradient problem**\n",
    "   - ReLU'(x) = 1 for x > 0 (no gradient decay)\n",
    "   - Gradients flow stably through all layers\n",
    "\n",
    "3. **Gradient distributions reveal the problem**\n",
    "   - Sigmoid: Distribution collapses near zero\n",
    "   - ReLU: Distribution stays healthy\n",
    "\n",
    "**Key takeaway:** For deep networks, **always use ReLU** (or variants like Leaky ReLU, ELU)!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Detecting Overfitting \n",
    "\n",
    "## What is overfitting?\n",
    "\n",
    "Overfitting occurs when a model **memorizes** training data instead of learning the underlying pattern.\n",
    "\n",
    "**Symptoms:**\n",
    "- Training loss ↓↓↓ (keeps decreasing)\n",
    "- Validation loss ↑ (starts increasing!)\n",
    "- Large gap between train and validation loss\n",
    "\n",
    "## How do we detect it?\n",
    "\n",
    "By tracking **both** training and validation loss:\n",
    "- **Good**: Both decrease together → generalization\n",
    "- **Bad**: Train ↓, Valid ↑ → overfitting!\n",
    "\n",
    "## Already Done! \n",
    "\n",
    "We actually already demonstrated overfitting detection in Part 1:\n",
    "- Generated separate train/validation/test sets\n",
    "- Tracked both training and validation loss\n",
    "- Plots show the gap between train and validation curves\n",
    "\n",
    "**Look at the previous plots:**\n",
    "- The \"Train vs. Validation Loss\" panel shows both curves\n",
    "- The gap between them indicates overfitting level\n",
    "- Final gap is reported (e.g., \"Gap: +0.7 Overfitting\")\n",
    "\n",
    "Now let's see how **regularization** can reduce overfitting!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Regularization Techniques\n",
    "\n",
    "## What is regularization?\n",
    "\n",
    "Regularization prevents overfitting by:\n",
    "- Constraining model complexity\n",
    "- Keeping weights small\n",
    "- Adding controlled randomness\n",
    "\n",
    "## Three techniques:\n",
    "\n",
    "### 1. L1 Regularization (Lasso)\n",
    "Adds penalty: $\\lambda_1 \\sum_i |w_i|$\n",
    "- Promotes **sparse** weights (many → 0)\n",
    "- Feature selection\n",
    "\n",
    "### 2. L2 Regularization (Ridge/Weight Decay)\n",
    "Adds penalty: $\\lambda_2 \\sum_i w_i^2$\n",
    "- Keeps weights **small**\n",
    "- Smooth solutions\n",
    "\n",
    "### 3. Dropout\n",
    "Randomly drops neurons during training:\n",
    "- Prevents co-adaptation\n",
    "- Ensemble-like behavior\n",
    "\n",
    "Let's compare them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1: Train WITH L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" PART 3: REGULARIZATION TECHNIQUES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[STEP 3.1] Training WITH L1 regularization...\")\n",
    "\n",
    "# Model without regularization\n",
    "model_l1 = MultiLayerPerceptron(\n",
    "    layer_sizes=architecture_deep,\n",
    "    activation='relu',\n",
    "    dropout_rate=0.0\n",
    ")\n",
    "\n",
    "# Train\n",
    "_, history_l1 = train_model_with_validation_tracking(\n",
    "    model=model_l1,\n",
    "    x_train=x_t,\n",
    "    y_train=y_t,\n",
    "    x_valid=x_valid_norm,\n",
    "    y_valid=y_valid,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    reg_type='l1',\n",
    "    lambda_l1=lambda_l1,\n",
    "    print_every=500,\n",
    "    track_gradients=False  # Speed up training\n",
    ")\n",
    "\n",
    "print(\"\\n L1 model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: Train WITH L2 Regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[STEP 3.2] Training WITH L2 regularization...\")\n",
    "print(f\"L2 penalty: {lambda_l2}\\n\")\n",
    "\n",
    "# Model with L2\n",
    "model_l2 = MultiLayerPerceptron(\n",
    "    layer_sizes=architecture_deep,\n",
    "    activation='relu',\n",
    "    dropout_rate=0.0\n",
    ")\n",
    "\n",
    "# Train with L2\n",
    "_, history_l2 = train_model_with_validation_tracking(\n",
    "    model=model_l2,\n",
    "    x_train=x_t,\n",
    "    y_train=y_t,\n",
    "    x_valid=x_valid_norm,\n",
    "    y_valid=y_valid,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    reg_type='l2',\n",
    "    lambda_l2=lambda_l2,\n",
    "    print_every=500,\n",
    "    track_gradients=False\n",
    ")\n",
    "\n",
    "print(\"\\n L2 regularized model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.3: Train WITH Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[STEP 3.3] Training WITH dropout...\")\n",
    "print(f\"Dropout rate: {dropout_rate}\\n\")\n",
    "\n",
    "# Model with dropout\n",
    "model_dropout = MultiLayerPerceptron(\n",
    "    layer_sizes=architecture_deep,\n",
    "    activation='relu',\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "\n",
    "# Train\n",
    "_, history_dropout = train_model_with_validation_tracking(\n",
    "    model=model_dropout,\n",
    "    x_train=x_t,\n",
    "    y_train=y_t,\n",
    "    x_valid=x_valid_norm,\n",
    "    y_valid=y_valid,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    reg_type='none',\n",
    "    print_every=500,\n",
    "    track_gradients=False\n",
    ")\n",
    "\n",
    "print(\"\\n Dropout model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.4: Compare Regularization Methods \n",
    "\n",
    "Now let's visualize all three approaches side-by-side!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"\\n[STEP 3.4] Comparing regularization methods...\\n\")\n",
    "\n",
    "# Compare all methods\n",
    "#plot_regularization_comparison(\n",
    "#    histories=[history_no_reg, history_l2, history_dropout],\n",
    "#    model_names=['No Regularization', 'L2 Regularization', f'Dropout (p={dropout_rate})'],\n",
    "#    save_name='regularization_comparison.png'\n",
    "#)\n",
    "\n",
    "#print(\"\\n Key Observations:\")\n",
    "#print(\"   - No regularization: Larger train-validation gap\")\n",
    "#print(\"   - L2: Smaller gap, smoother training\")\n",
    "#print(\"   - Dropout: Smaller gap, more stable\")\n",
    "#print(\"\\n   Regularization keeps validation loss close to training loss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.5: Visualize Dropout Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[STEP 3.5] Visualizing dropout model fit...\")\n",
    "\n",
    "plot_results(\n",
    "    x_t, y_t,\n",
    "    relu_model,\n",
    "    relu_history,\n",
    "    coeffs_true,\n",
    "    poly_order,\n",
    "    model_name=f\"Deep MLP with no reg.\",\n",
    "    normalizer=normalizer,\n",
    "    show_validation=True\n",
    ")\n",
    "\n",
    "\n",
    "plot_results(\n",
    "    x_t, y_t,\n",
    "    model_l1,\n",
    "    history_l1,\n",
    "    coeffs_true,\n",
    "    poly_order,\n",
    "    model_name=f\"Deep MLP with L1 (L1={lambda_l1}).\",\n",
    "    normalizer=normalizer,\n",
    "    show_validation=True\n",
    ")\n",
    "\n",
    "plot_results(\n",
    "    x_t, y_t,\n",
    "    model_l2,\n",
    "    history_l2,\n",
    "    coeffs_true,\n",
    "    poly_order,\n",
    "    model_name=f\"Deep MLP with L2 (L2={lambda_l2})\",\n",
    "    normalizer=normalizer,\n",
    "    show_validation=True\n",
    ")\n",
    "\n",
    "plot_results(\n",
    "    x_t, y_t,\n",
    "    model_dropout,\n",
    "    history_dropout,\n",
    "    coeffs_true,\n",
    "    poly_order,\n",
    "    model_name=f\"Deep MLP with Dropout (p={dropout_rate})\",\n",
    "    normalizer=normalizer,\n",
    "    show_validation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Summary\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "1.  **Without regularization**: Models can overfit\n",
    "   - Training loss keeps decreasing\n",
    "   - Validation loss plateaus or increases\n",
    "   - Large train-validation gap\n",
    "\n",
    "2.  **L2 regularization**: Keeps weights small\n",
    "   - Adds penalty: $\\lambda_2 \\sum w_i^2$\n",
    "   - Reduces overfitting\n",
    "   - Smoother training curves\n",
    "\n",
    "3.  **Dropout**: Random neuron deactivation\n",
    "   - Prevents co-adaptation\n",
    "   - Acts like ensemble learning\n",
    "   - Very effective regularization\n",
    "\n",
    "**Key takeaway:** Always use regularization for better generalization!\n",
    "\n",
    "**Recommended approach:**\n",
    "- Start with L2 regularization (λ ≈ 0.01)\n",
    "- Add dropout (p ≈ 0.2-0.3) for extra robustness\n",
    "- Monitor train-validation gap to tune hyperparameters\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Tutorial Complete!\n",
    "\n",
    "## Congratulations! \n",
    "\n",
    "You've mastered the fundamentals of deep neural networks:\n",
    "\n",
    "### What you learned:\n",
    "\n",
    "1. **Vanishing Gradients**\n",
    "   - Why sigmoid/tanh fail in deep networks\n",
    "   - How ReLU solves the problem\n",
    "   - Gradient flow visualization\n",
    "\n",
    "2. **Overfitting Detection**\n",
    "   - Train/validation/test splits\n",
    "   - Monitoring train-validation gap\n",
    "   - Identifying memorization vs learning\n",
    "\n",
    "3. **Regularization**\n",
    "   - L2 regularization (weight decay)\n",
    "   - Dropout (random deactivation)\n",
    "   - Preventing overfitting\n",
    "\n",
    "###  Experiments to Try:\n",
    "\n",
    "1. **Change network depth**: Try 5 layers vs 15 layers\n",
    "2. **Modify data complexity**: Change `poly_order` to 5 or 15\n",
    "3. **Tune regularization**: Try different λ values (0.001, 0.1, 1.0)\n",
    "4. **Compare learning rates**: Test [0.001, 0.01, 0.1]\n",
    "5. **Add a skip connection between the layers**\n",
    "\n",
    "###  Next Steps:\n",
    "\n",
    "- **Tutorial 3**: [From DNNs to Transformers](../docs/tutorials/tutorial-3.md)\n",
    "\n",
    "###  Key Principles to Remember:\n",
    "\n",
    "1. **Always use ReLU** (or variants) for deep networks\n",
    "2. **Always split data** into train/val/test\n",
    "3. **Always monitor validation loss** to detect overfitting\n",
    "4. **Always use regularization** (L2 + dropout is a good default)\n",
    "5. **Always normalize inputs** before training\n",
    "\n",
    "---\n",
    "\n",
    "##  Questions?\n",
    "\n",
    "- Check the [FAQ](../docs/faq.md)\n",
    "- See [Troubleshooting](../docs/troubleshooting.md)\n",
    "- Open an issue on GitHub\n",
    "\n",
    "Happy learning! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tutorial Environment",
   "language": "python",
   "name": "tutorial-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
