{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Polynomial Regression & Bias-Variance Tradeoff\n",
    "\n",
    "**Author:** ML Tutorial Series  \n",
    "**Target Audience:** PhD students and early career postdocs  \n",
    "**Duration:** 90-120 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial introduces the fundamentals of PyTorch and machine learning through **polynomial regression**, demonstrating the critical **bias-variance tradeoff**.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "\n",
    "- Understand PyTorch tensor operations and automatic differentiation\n",
    "- Learn to create custom model classes using `nn.Module`\n",
    "- Implement forward passes and parameter optimization for arbitrary polynomial orders\n",
    "- Grasp the training loop structure and parameter evolution\n",
    "- **Visualize and understand the bias-variance tradeoff**\n",
    "- Recognize underfitting (high bias) and overfitting (high variance)\n",
    "- Learn when model complexity matches data complexity\n",
    "- Understand feature normalization for numerical stability\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Underfitting (High Bias):** Model too simple to capture the true function\n",
    "- **Good Fit (Balanced):** Model complexity matches data complexity\n",
    "- **Overfitting (High Variance):** Model too complex, fits noise instead of signal\n",
    "- **Feature Normalization:** Critical for numerical stability with high-order polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup and Imports\n",
    "\n",
    "Let's start by importing all necessary libraries and configuring our logging system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Tutorial imports\n",
    "from polynomial_tutorial.LinearRegressor import LinearRegressor, PolynomialRegressor\n",
    "from polynomial_tutorial.train import train_model, evaluate_model\n",
    "from polynomial_tutorial.utils import FeatureNormalizer\n",
    "from polynomial_tutorial.logger import configure_logging\n",
    "\n",
    "# Configure logging\n",
    "configure_logging()\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('default')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úì All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Configuration\n",
    "\n",
    "### Experiment Configuration\n",
    "\n",
    "The bias-variance tradeoff depends on the relationship between:\n",
    "- **Data polynomial order:** Complexity of the true underlying function\n",
    "- **Regressor polynomial order:** Complexity of the model we fit\n",
    "\n",
    "**Three scenarios:**\n",
    "1. `regressor_order < data_order` ‚Üí **Underfitting** (high bias)\n",
    "2. `regressor_order = data_order` ‚Üí **Good fit** (balanced)\n",
    "3. `regressor_order > data_order` ‚Üí **Overfitting** (high variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION: Experiment with these parameters!\n",
    "# ============================================================================\n",
    "\n",
    "# TRUE DATA GENERATION\n",
    "data_poly_order = 3  # Order of the TRUE underlying function\n",
    "coeffs_true = [1.0, 0.5, -0.3, 0.1]  # True coefficients [a‚ÇÄ, a‚ÇÅ, a‚ÇÇ, a‚ÇÉ]\n",
    "\n",
    "# MODEL CONFIGURATION  \n",
    "regressor_poly_order = 3  # Order of the MODEL to fit\n",
    "\n",
    "# TRAINING CONFIGURATION\n",
    "n_samples = 100\n",
    "noise_std = 0.5\n",
    "num_epochs = 2000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# FEATURE NORMALIZATION (automatic for orders > 2)\n",
    "use_normalization = max(data_poly_order, regressor_poly_order) > 2\n",
    "normalization_method = 'symmetric'  # Maps to [-1, 1]\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Data polynomial order: {data_poly_order}\")\n",
    "print(f\"  Regressor polynomial order: {regressor_poly_order}\")\n",
    "print(f\"  Use normalization: {use_normalization}\")\n",
    "print(f\"  Samples: {n_samples}, Noise std: {noise_std}\")\n",
    "print(f\"  Epochs: {num_epochs}, Learning rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Data Generation\n",
    "\n",
    "### Understanding the True Function\n",
    "\n",
    "We generate data from a polynomial function:\n",
    "\n",
    "$$y = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ is Gaussian noise.\n",
    "\n",
    "This noise creates the **irreducible error** ‚Äî the best possible model still has error equal to $\\sigma^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(coeffs_true, data_poly_order, n_samples=100, \n",
    "                  x_min=0, x_max=10, noise_std=0.5):\n",
    "    \"\"\"\n",
    "    Generate synthetic polynomial data with noise.\n",
    "    \n",
    "    Args:\n",
    "        coeffs_true: True polynomial coefficients [a‚ÇÄ, a‚ÇÅ, ..., a‚Çô]\n",
    "        data_poly_order: Polynomial order\n",
    "        n_samples: Number of data points\n",
    "        x_min, x_max: Range for x values\n",
    "        noise_std: Standard deviation of Gaussian noise\n",
    "    \n",
    "    Returns:\n",
    "        x_t, y_t: Input and noisy output tensors\n",
    "    \"\"\"\n",
    "    # Validate\n",
    "    assert len(coeffs_true) == data_poly_order + 1\n",
    "    \n",
    "    # Generate x values uniformly\n",
    "    x = torch.linspace(x_min, x_max, n_samples)\n",
    "    \n",
    "    # Compute true y values\n",
    "    y_true = torch.zeros(n_samples)\n",
    "    for i, coeff in enumerate(coeffs_true):\n",
    "        y_true += coeff * (x ** i)\n",
    "    \n",
    "    # Add Gaussian noise\n",
    "    noise = torch.randn(n_samples) * noise_std\n",
    "    y_noisy = y_true + noise\n",
    "    \n",
    "    return x, y_noisy\n",
    "\n",
    "\n",
    "# Generate data\n",
    "logger.info(\"\\n[STEP 1] Generating synthetic polynomial data...\")\n",
    "logger.info(f\"Data polynomial order: {data_poly_order}\")\n",
    "logger.info(f\"True coefficients: {coeffs_true}\")\n",
    "\n",
    "x_t, y_t = generate_data(\n",
    "    coeffs_true=coeffs_true,\n",
    "    data_poly_order=data_poly_order,\n",
    "    n_samples=n_samples,\n",
    "    x_min=0,\n",
    "    x_max=10,\n",
    "    noise_std=noise_std\n",
    ")\n",
    "\n",
    "logger.info(f\"Generated {n_samples} data points\")\n",
    "logger.info(f\"x range: [{x_t.min():.2f}, {x_t.max():.2f}]\")\n",
    "logger.info(f\"y range: [{y_t.min():.2f}, {y_t.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot of noisy data\n",
    "plt.scatter(x_t.numpy(), y_t.numpy(), alpha=0.5, s=30, label='Observed data')\n",
    "\n",
    "# Plot true function (without noise)\n",
    "x_dense = torch.linspace(0, 10, 200)\n",
    "y_true_dense = torch.zeros(200)\n",
    "for i, coeff in enumerate(coeffs_true):\n",
    "    y_true_dense += coeff * (x_dense ** i)\n",
    "\n",
    "plt.plot(x_dense.numpy(), y_true_dense.numpy(), 'g--', \n",
    "         linewidth=2, label='True function (no noise)')\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Generated Data: Polynomial with Noise', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Data generated: {n_samples} points\")\n",
    "print(f\"  True function: y = {coeffs_true[0]:.2f} + {coeffs_true[1]:.2f}x + {coeffs_true[2]:.2f}x¬≤ + {coeffs_true[3]:.2f}x¬≥\")\n",
    "print(f\"  Noise level (œÉ): {noise_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Feature Normalization\n",
    "\n",
    "### Why Normalization is Critical\n",
    "\n",
    "When $x \\in [0, 10]$ and we compute polynomial features:\n",
    "\n",
    "| Feature | Range | Magnitude |\n",
    "|---------|-------|----------|\n",
    "| $x^0$ | 1 | $10^0$ |\n",
    "| $x^1$ | [0, 10] | $10^1$ |\n",
    "| $x^2$ | [0, 100] | $10^2$ |\n",
    "| $x^3$ | [0, 1000] | $10^3$ ‚ö†Ô∏è |\n",
    "| $x^4$ | [0, 10,000] | $10^4$ üö® |\n",
    "| $x^5$ | [0, 100,000] | $10^5$ üí• |\n",
    "\n",
    "**Problem:** Gradients explode ‚Üí NaN values ‚Üí training fails!\n",
    "\n",
    "**Solution:** Normalize $x$ to $[-1, 1]$ **before** computing powers:\n",
    "- $(x_{norm})^3$ stays in $[-1, 1]$ ‚úÖ\n",
    "- $(x_{norm})^5$ stays in $[-1, 1]$ ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply normalization if needed\n",
    "normalizer = None\n",
    "\n",
    "if use_normalization:\n",
    "    logger.info(\"\\n[STEP 1.5] Applying feature normalization...\")\n",
    "    logger.info(f\"‚ö†Ô∏è  Polynomial order {max(data_poly_order, regressor_poly_order)} requires normalization!\")\n",
    "    \n",
    "    normalizer = FeatureNormalizer(method=normalization_method)\n",
    "    x_t_original = x_t.clone()\n",
    "    x_t = normalizer.fit_transform(x_t)\n",
    "    \n",
    "    logger.info(f\"Before: x ‚àà [{x_t_original.min():.2f}, {x_t_original.max():.2f}]\")\n",
    "    logger.info(f\"After:  x ‚àà [{x_t.min():.2f}, {x_t.max():.2f}]\")\n",
    "    logger.info(\"‚úì Features normalized to [-1, 1]\")\n",
    "    \n",
    "    # Demonstrate the effect\n",
    "    print(\"\\nNormalization Effect:\")\n",
    "    for order in [1, 2, 3, 4, 5]:\n",
    "        x_power = x_t ** order\n",
    "        print(f\"  x^{order}: range [{x_power.min():.2f}, {x_power.max():.2f}]\")\n",
    "else:\n",
    "    logger.info(\"\\n[STEP 1.5] Skipping normalization (order ‚â§ 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Model Definition\n",
    "\n",
    "### The PolynomialRegressor Class\n",
    "\n",
    "Our model implements:\n",
    "\n",
    "$$y = a_0 + a_1 x + a_2 x^2 + \\cdots + a_n x^n$$\n",
    "\n",
    "**Key features:**\n",
    "- Arbitrary polynomial order\n",
    "- Learnable coefficients as `nn.Parameter`\n",
    "- Forward pass computes polynomial features and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "logger.info(\"\\n[STEP 2] Initializing the polynomial regression model...\")\n",
    "logger.info(f\"Regressor polynomial order: {regressor_poly_order}\")\n",
    "\n",
    "model = PolynomialRegressor(order=regressor_poly_order)\n",
    "logger.info(f\"Model initialized with {regressor_poly_order + 1} parameters\")\n",
    "\n",
    "# Analyze bias-variance scenario\n",
    "if regressor_poly_order < data_poly_order:\n",
    "    scenario = \"UNDERFITTING\"\n",
    "    logger.info(\"‚ö†Ô∏è  UNDERFITTING: Model order < Data order (High Bias)\")\n",
    "elif regressor_poly_order == data_poly_order:\n",
    "    scenario = \"GOOD FIT\"\n",
    "    logger.info(\"‚úì GOOD FIT: Model order = Data order (Balanced)\")\n",
    "else:\n",
    "    scenario = \"OVERFITTING RISK\"\n",
    "    logger.info(\"‚ö†Ô∏è  OVERFITTING RISK: Model order > Data order (High Variance)\")\n",
    "\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"  Architecture: Polynomial regression (order {regressor_poly_order})\")\n",
    "print(f\"  Parameters: {regressor_poly_order + 1} coefficients\")\n",
    "print(f\"  Scenario: {scenario}\")\n",
    "print(f\"\\n{model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Training\n",
    "\n",
    "### The Training Loop\n",
    "\n",
    "Standard gradient descent optimization:\n",
    "\n",
    "1. **Forward pass:** Compute predictions\n",
    "2. **Compute loss:** Mean Squared Error (MSE)\n",
    "3. **Backward pass:** Compute gradients via autograd\n",
    "4. **Update:** $\\theta \\leftarrow \\theta - \\alpha \\nabla L(\\theta)$\n",
    "\n",
    "**Special:** We track parameter evolution to visualize convergence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "logger.info(\"\\n[STEP 3] Training the model...\")\n",
    "logger.info(f\"Training on {'NORMALIZED' if use_normalization else 'ORIGINAL'} features\")\n",
    "\n",
    "trained_model, training_history = train_model(\n",
    "    model=model,\n",
    "    x_train=x_t,\n",
    "    y_train=y_t,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    print_every=400,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Training completed!\")\n",
    "print(f\"  Final loss: {training_history['loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Evaluation\n",
    "\n",
    "### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"\\n[STEP 4] Evaluating results...\")\n",
    "\n",
    "fitted_params = trained_model.get_parameters()\n",
    "\n",
    "if use_normalization:\n",
    "    logger.info(\"\\n‚ö†Ô∏è  Note: Coefficients are for NORMALIZED features\")\n",
    "    logger.info(\"Cannot directly compare to true coefficients (different scales)\")\n",
    "\n",
    "print(f\"\\nTrue coefficients (original scale):   {coeffs_true}\")\n",
    "print(f\"Fitted coefficients (working scale):  {[f'{p:.4f}' for p in fitted_params]}\")\n",
    "\n",
    "# Compute metrics\n",
    "metrics = evaluate_model(trained_model, x_t, y_t)\n",
    "print(f\"\\nFinal Metrics:\")\n",
    "print(f\"  MSE:  {metrics['mse']:.6f}\")\n",
    "print(f\"  RMSE: {metrics['rmse']:.6f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  Noise level (œÉ): {noise_std}\")\n",
    "print(f\"  RMSE ‚âà œÉ? {abs(metrics['rmse'] - noise_std) < 0.1} (good fit if True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Visualization - Parameter Evolution\n",
    "\n",
    "### How Parameters Converge During Training\n",
    "\n",
    "Watching parameters converge helps understand:\n",
    "- Which coefficients are important (converge to non-zero)\n",
    "- Which are less important (converge near zero)\n",
    "- Convergence speed differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parameter_evolution(training_history, coeffs_true, model):\n",
    "    \"\"\"Visualize parameter convergence.\"\"\"\n",
    "    param_history = training_history['params']\n",
    "    num_epochs = len(param_history)\n",
    "    \n",
    "    # Extract parameter array\n",
    "    param_array = np.array([epoch_params[0].detach().cpu().numpy() \n",
    "                            for epoch_params in param_history])\n",
    "    num_params = param_array.shape[1]\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, num_params, figsize=(5 * num_params, 4))\n",
    "    if num_params == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    epochs = np.arange(num_epochs)\n",
    "    \n",
    "    for i in range(num_params):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot evolution\n",
    "        ax.plot(epochs, param_array[:, i], 'b-', linewidth=2, \n",
    "                label=f'Estimated a_{i}')\n",
    "        \n",
    "        # Plot true value if available (and not normalized)\n",
    "        if i < len(coeffs_true) and not use_normalization:\n",
    "            ax.axhline(y=coeffs_true[i], color='g', linestyle='--', \n",
    "                      linewidth=2, label=f'True a_{i} = {coeffs_true[i]:.3f}')\n",
    "        \n",
    "        ax.set_xlabel('Epoch', fontsize=12)\n",
    "        ax.set_ylabel(f'Coefficient a_{i}', fontsize=12)\n",
    "        ax.set_title(f'Parameter a_{i} Evolution', fontsize=14)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n[VISUALIZATION 1] Parameter Evolution\")\n",
    "plot_parameter_evolution(training_history, coeffs_true, trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Visualization - Fitted Curve\n",
    "\n",
    "### Comparing Model Predictions to True Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(x_t, y_t, model, training_history, coeffs_true, \n",
    "                 data_poly_order, normalizer=None):\n",
    "    \"\"\"Visualize fitted curve and loss.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Denormalize for display if needed\n",
    "    if normalizer is not None:\n",
    "        x_display = normalizer.inverse_transform(x_t).detach().numpy()\n",
    "        # Dense points for smooth curve\n",
    "        x_dense_original = torch.linspace(x_display.min(), x_display.max(), 200)\n",
    "        x_dense_norm = normalizer.transform(x_dense_original)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_pred_dense = model(x_dense_norm).numpy()\n",
    "        \n",
    "        # True function\n",
    "        y_true_dense = torch.zeros_like(x_dense_original)\n",
    "        for i, coeff in enumerate(coeffs_true):\n",
    "            y_true_dense += coeff * (x_dense_original ** i)\n",
    "        \n",
    "        x_plot = x_dense_original.numpy()\n",
    "        y_pred_plot = y_pred_dense\n",
    "        y_true_plot = y_true_dense.numpy()\n",
    "    else:\n",
    "        x_display = x_t.detach().numpy()\n",
    "        with torch.no_grad():\n",
    "            y_pred_plot = model(x_t).numpy()\n",
    "        x_plot = x_display\n",
    "        y_true_plot = torch.zeros_like(x_t)\n",
    "        for i, coeff in enumerate(coeffs_true):\n",
    "            y_true_plot += coeff * (x_t ** i)\n",
    "        y_true_plot = y_true_plot.numpy()\n",
    "    \n",
    "    y_np = y_t.detach().numpy()\n",
    "    \n",
    "    # Plot 1: Fitted curve\n",
    "    axes[0].scatter(x_display, y_np, alpha=0.5, s=30, c='blue', label='Data')\n",
    "    axes[0].plot(x_plot, y_pred_plot, 'r-', linewidth=2.5, \n",
    "                 label=f'Fitted (order={model.order})')\n",
    "    axes[0].plot(x_plot, y_true_plot, 'g--', linewidth=2, \n",
    "                 label=f'True (order={data_poly_order})')\n",
    "    \n",
    "    # Scenario title\n",
    "    if model.order < data_poly_order:\n",
    "        title = \"UNDERFITTING (High Bias)\"\n",
    "    elif model.order == data_poly_order:\n",
    "        title = \"GOOD FIT (Balanced)\"\n",
    "    else:\n",
    "        title = \"POTENTIAL OVERFITTING (High Variance)\"\n",
    "    \n",
    "    axes[0].set_xlabel('x', fontsize=12)\n",
    "    axes[0].set_ylabel('y', fontsize=12)\n",
    "    axes[0].set_title(f'Polynomial Regression: {title}', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Loss curve\n",
    "    loss_history = training_history['loss']\n",
    "    axes[1].plot(loss_history, linewidth=2, color='darkblue')\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('MSE', fontsize=12)\n",
    "    axes[1].set_title('Training Loss', fontsize=14)\n",
    "    axes[1].set_yscale('log')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n[VISUALIZATION 2] Fitted Curve vs True Function\")\n",
    "plot_results(x_t, y_t, trained_model, training_history, \n",
    "             coeffs_true, data_poly_order, normalizer=normalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Understanding the Results\n",
    "\n",
    "### Interpreting the Bias-Variance Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\" BIAS-VARIANCE TRADEOFF SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nData Order:       {data_poly_order}\")\n",
    "print(f\"Regressor Order:  {regressor_poly_order}\")\n",
    "print(f\"Final MSE:        {training_history['loss'][-1]:.6f}\")\n",
    "print(f\"RMSE:             {metrics['rmse']:.6f}\")\n",
    "print(f\"Noise level (œÉ):  {noise_std}\")\n",
    "\n",
    "print(\"\\nINTERPRETATION:\")\n",
    "\n",
    "if regressor_poly_order < data_poly_order:\n",
    "    print(\"\"\"\n",
    "üî¥ UNDERFITTING (High Bias, Low Variance)\n",
    "- Model is too simple to capture the true function\n",
    "- Cannot fit the training data well\n",
    "- Both training and test error will be high\n",
    "- Bias dominates the error\n",
    "\n",
    "Solution: Increase model complexity (higher polynomial order)\n",
    "\"\"\")\n",
    "elif regressor_poly_order == data_poly_order:\n",
    "    print(\"\"\"\n",
    "üü¢ GOOD FIT (Balanced Bias-Variance)\n",
    "- Model complexity matches data complexity\n",
    "- Can fit the training data well\n",
    "- Generalizes well to unseen data\n",
    "- Optimal tradeoff point\n",
    "\n",
    "This is the ideal scenario!\n",
    "\"\"\")\n",
    "else:\n",
    "    print(\"\"\"\n",
    "üü° OVERFITTING RISK (Low Bias, High Variance)\n",
    "- Model is too complex for the data\n",
    "- Fits training data very well (even noise)\n",
    "- May generalize poorly to new data\n",
    "- Variance dominates the error\n",
    "\n",
    "Solution: Reduce model complexity or add regularization\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 11: Experiments to Try\n",
    "\n",
    "### Experiment 1: Underfitting\n",
    "\n",
    "Try fitting a **linear model** (order 1) to **cubic data** (order 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run to try underfitting\n",
    "# data_poly_order = 3\n",
    "# regressor_poly_order = 1  # Too simple!\n",
    "# coeffs_true = [1.0, 0.5, -0.3, 0.1]\n",
    "\n",
    "# Then re-run cells from Part 3 onward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Overfitting\n",
    "\n",
    "Try fitting a **6th order** model to **quadratic data** (order 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run to try overfitting\n",
    "# data_poly_order = 2\n",
    "# regressor_poly_order = 6  # Too complex!\n",
    "# coeffs_true = [1.0, 0.5, -0.3]\n",
    "\n",
    "# Then re-run cells from Part 3 onward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Noise Sensitivity\n",
    "\n",
    "How does noise affect overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different noise levels\n",
    "noise_levels = [0.1, 0.5, 1.0, 2.0]\n",
    "\n",
    "# Experiment: For each noise level, train overfitted model\n",
    "# Question: How does noise affect the bias-variance tradeoff?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 12: Key Takeaways\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "1. **Bias-Variance Tradeoff:**\n",
    "   - Model complexity must match data complexity\n",
    "   - Too simple ‚Üí high bias (underfitting)\n",
    "   - Too complex ‚Üí high variance (overfitting)\n",
    "   - Just right ‚Üí balanced (good generalization)\n",
    "\n",
    "2. **Feature Normalization:**\n",
    "   - Critical for polynomial orders > 2\n",
    "   - Prevents numerical instability (NaN values)\n",
    "   - Normalize **before** computing polynomial features\n",
    "   - Denormalize for interpretable plots\n",
    "\n",
    "3. **PyTorch Fundamentals:**\n",
    "   - Creating custom `nn.Module` classes\n",
    "   - Training loops with autograd\n",
    "   - Parameter tracking and visualization\n",
    "   - Professional logging practices\n",
    "\n",
    "4. **The Goal is NOT Minimum Training Loss!**\n",
    "   - Lowest training loss can indicate overfitting\n",
    "   - Goal: Build models that **generalize to new data**\n",
    "   - RMSE ‚âà noise level œÉ indicates good fit\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try different polynomial orders\n",
    "- Experiment with noise levels\n",
    "- Implement cross-validation for model selection\n",
    "- Add L2 regularization (Ridge regression)\n",
    "- Move to Tutorial 2: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Congratulations! üéâ\n",
    "\n",
    "You've completed Tutorial 1 and learned:\n",
    "- ‚úÖ Polynomial regression with PyTorch\n",
    "- ‚úÖ The bias-variance tradeoff\n",
    "- ‚úÖ Feature normalization for numerical stability\n",
    "- ‚úÖ Professional ML coding practices\n",
    "\n",
    "**Ready for more?** Proceed to Tutorial 2: Perceptron to Deep Neural Networks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
